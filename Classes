31.07
- statisticians are into inference, for hypothesis testing
- mean and distribution is not sufficient for them
- IS IT SIGNIFICANT? 

Dummy regression: 
X = [M, F] X' = transposed X
multiplying gives 2x2 matrix
  [ 1\avg_M       0   ]
  [ 0         1\avg_F ]
inverting ^-1 => change diagonal 

several ways to obtain results, specifications is same, but interpretation changes
=> reparametrization
a without hat is conditional expectation of wage given gender is female = constant
-> average for females

With dummys M = 1 - F, multiplying 1-F out then second constant appears that gets added to previous intercept 
then constant = average for males

further reparametrization by substituting constant with F+M (rhs of note sheet of teacher)
reg wage female male, nocons
no reference category needed, means already displayed

reference category = baseline 

dummies and constant => dummy variable trap and thus one dummy get dropped
no perfect linear collinearity
is not violated since:
M, F and 1 has a linear relationship and 1 is dropped thus works
OR
full rank condition holds: my scalars in the matrix are not multiples of each other -> linearly independent

M, F and 1 are 3 ranks
but one will be dropped of these three, only two left
3 possibilities 
1) constant + male coeff
2) constant + female coeff
3) male coef + female coeff

------------------------------------------------------------------------------------------------------
r^2 = ESS\TSS = 1 - RSS\TSS = [SUM ( y^hat - avg_y)^2] / SUM (y - avg_y)^2]
proportion explained by my regressors 
- regressand y^hat obtained from regression

iv. this student is violating the rank condition

------------------------------------------------------------------------------------------------------
Hypothesis testing - do it for the inference 
distribution of y is determining what happens in regressions
if y are normal beta^hat normality can come

never make hats on ttesting -> doesnt make sense 
average probability to jump around -> st dev
- y is normal, beta is normal thus ~N(0,1)
- combination of linear combinations results in a normal distri


STATA reg: std error is hatted standard error based on beta^hat

P>|t| 
!= significance level
!= probability that the H0 is true
= probability that coefficient 
p value probabilitz under assumption and null that i obtain a test statistic \\
more unfavorable to the null than i actually obtained
=> smallest level of significance at which zou can reject the null
t is telling where we ended on xaxis of the distribution



DoF = N - k 


02.08.2023
If you have no clue:
1) sign
2) magnitude
3) Significance

Sometimes two variables singularly display no effect, but jointly they do
All regressors apart from coas are not significant (at 5%), yet the F test is 
significant. Normally we suspect multicollinearity, but can 
also be heteroskedasticity, because that will invalidate the F test as well 
as t tests. 

Heteroskedasticity: about variance. distribution of variance -> 
more consistent same distribution over x but not necessarily normal!!
Var(epsilon|X) proportional to x_ji 
standard formula of OLS would suffer by this -> not anymore efficient 
in Gauss Markov terms not best anymore!!
Summed: 
1) loss of efficiency
2) invalid standard error -> problematic for drawing inference

Detect Heteroskedasticity with Goldfeld-Quandt test 
split vertically data and look if RSS differs significantly between two sides,
to see if dispersion is different between eg low and high variables
If cone is open toward yaxis then flip RSS conversly to cone that is pointy at low x and opening toward larger x
Drawback that only for vector it makes sense to split in subset, for matrix difficult 

1 - type 2 error = power of test
deleting middle part if split into 3 to increase power of test as bottom tertio and upper are the more extreme parts at
least if it has cone form
to gain power you can switch from two sided two one sided test

Even under Heteroskedasticity OLS remains consistent

RSS under normality will be chi squred distributed 

var(beta hat) = sigma ^2(X'X)^-1
not constant as sigma^2 not constant if heteroskedasticity 

Heteroskedasticitz var: (X'X)^-1 var(epsiolon|X) X (X'X) ^-1
computing this is and doing what discussed in class is what robust standard errors

spericity = var(epsilon|X) = sigma^2*I^2_N
Plug this into heterosk var and lift sigma in front then I can be built of X 
and as a result the var(beta hat) is like in the homosk case

Breuschan Pagan: 
y usuallz subset of regressors
epsilon hats still valid by OLS, se are flawed NOT epsilon

LM test n*R^2 ~ chi squared_(dim alpha1) 

White generalized Breuschan Pagan test by using higher order polynomials (squared and interactions) 
that follows in principle the taylore series expansion
- dummy var does not need to be squared as its already dummy!!!!
- conversly, residuals should be squared 
- vectors cannot be squared but the values of the vector can be
- can be problematic if N low but k high and by WHite loosing more k 


4/8. 
Difference between OLS and MLE
- MLE requires density function with joint distribution assumption

MLE: 
PROBABILITY =
- Given parameters (theta) find probabilities of 1 
- theta and y are vectors
- integrating cdf gives pdf

STATS = 
Given data y find plausible theta
-> flipped approach, we can use probability approach to determine stats problem


Using joint distribution of y to determine 
maximising log L 


f(y; theta) = conceptually (theta, y) 
on rhs no density term, as in statistic with actual data we do not have random data! 
Random data in probability allows us to talk about distribution












